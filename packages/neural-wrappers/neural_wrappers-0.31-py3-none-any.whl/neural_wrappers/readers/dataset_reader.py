from __future__ import annotations
import numpy as np
from abc import ABC, abstractmethod
from typing import Dict, List, Callable, Any, Iterator, Union, Tuple
from copy import deepcopy

from .dataset_types import DimGetterCallable, DimTransformCallable, DatasetIndex, DatasetItem
from .dataset_format import DatasetFormat

# Iterator that iterates one epoch over this dataset.
# @brief Epoch iterator that goes through the provided dataset reader for exactly one epoch as defined by len(reader)
# @param[in] reader The DatasetReader we are iterating one epoch upon
class DatasetEpochIterator:
	def __init__(self, reader:DatasetReader):
		self.reader = reader
		self.ix = -1
		self.len = len(self.reader)
	
	def __len__(self):
		return self.len

	def __getitem__(self, ix):
		return self.reader[ix]

	# The logic of getting an item is. ix is a number going in range [0 : len(self) - 1]. Then, we call dataset's
	#  __getitem__ on this. So item = self[index], where __getitem__(ix) = self.reader[ix].
	# One-liner: items = self[ix] for ix in [0 : len(self) - 1]
	def __next__(self):
		self.ix += 1
		if self.ix < len(self):
			return self.__getitem__(self.ix)
		raise StopIteration

	def __iter__(self):
		return self

class DatasetReader(ABC):
	def __init__(self, dataBuckets:Dict[str, List[str]], dimGetter:Dict[str, DimGetterCallable], \
		dimTransform:Dict[str, Dict[str, DimTransformCallable]]):
		self.datasetFormat = DatasetFormat(dataBuckets, dimGetter, dimTransform)

	@abstractmethod
	def getDataset(self) -> Any:
		pass

	@abstractmethod
	def __len__(self) -> int:
		pass

	# Public interface

	# @brief The main iterator of a dataset. It will run over the data for one logical epoch.
	def iterateOneEpoch(self) -> Iterator[Dict[str, Any]]:
		return DatasetEpochIterator(self)

	# Generic infinite generator, that simply does a while True over the iterate_once method, which only goes one epoch
	# @param[in] type The type of processing that is generated by the generator (typicall train/test/validation)
	# @param[in] maxPrefetch How many items in advance to be generated and stored before they are consumed. If 0, the
	#  thread API is not used at all. If 1, the thread API is used with a queue of length 1 (still works better than
	#  normal in most cases, due to the multi-threaded nature. For length > 1, the queue size is just increased.
	def iterateForever(self, maxPrefetch:int=0) -> Iterator[Dict[str, np.ndarray]]:
		assert maxPrefetch >= 0
		from .dataset_generator import DatasetGenerator
		return DatasetGenerator(self, maxPrefetch)

	def iterate(self):
		return self.iterateForever()

	# Used by CachedDatasetReader to cache a key. By default, we call str(key), but this can be overriden by readers.
	def cacheKey(self, key):
		return str(key)

	# We just love to reinvent the wheel. But also let's reuse the existing wheels just in case.
	def __str__(self) -> str:
		summaryStr = "[Dataset Reader]"
		summaryStr += "\n - Type: %s" % type(self)
		summaryStr += "\n - Data buckets:"
		for dataBucket in self.datasetFormat.dataBuckets:
			summaryStr += "\n   - %s => %s" % (dataBucket, self.datasetFormat.dataBuckets[dataBucket])
		summaryStr += "\n - Len: %d" % len(self)
		return summaryStr

	# @brief Returns the item at index i. Basically g(i) -> Item(i). Item(i) will follow dataBuckets schema,
	#  and will call dimGetter for each dimension for this index.
	# @return The item at index i
	def __getitem__(self, index):
		dataBuckets = self.datasetFormat.dataBuckets
		allDims = self.datasetFormat.allDims
		dimGetter = self.datasetFormat.dimGetter
		dimTransforms = self.datasetFormat.dimTransform
		dataset = self.getDataset()
		dimToDataBuckets = self.datasetFormat.dimToDataBuckets

		# The result is simply a dictionary that follows the (shallow, for now) dataBuckets of format.
		result = {k : {k2 : None for k2 in dataBuckets[k]} for k in dataBuckets}
		# rawItems = {k : None for k in allDims}
		for dim in allDims:
			getterFn = dimGetter[dim]
			# Call the getter only once for efficiency
			rawItem = getterFn(dataset, index)
			# rawItems[dim] = rawItem
			# Call the transformer for each data bucket independently (labels and data may use same
			#  dim but do a different transformation (such as normalized in data and unnormalized in
			#  labels for metrics or plotting or w/e.
			for bucket in dimToDataBuckets[dim]:
				transformFn = dimTransforms[bucket][dim]
				item = transformFn(deepcopy(rawItem))
				result[bucket][dim] = item
		return result

	def __iter__(self):
		return self.iterateOneEpoch()