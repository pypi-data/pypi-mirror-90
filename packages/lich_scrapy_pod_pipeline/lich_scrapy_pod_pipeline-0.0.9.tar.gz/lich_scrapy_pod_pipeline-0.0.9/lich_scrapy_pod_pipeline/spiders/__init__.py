# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.

import scrapy
from os_scrapy_linkextractor.lx_extensions import LxExtensionManager
from os_scrapy_record.items import fetch_record


class ExampleSpider(scrapy.Spider):
    """ ExampleSpider
    Auto generated by os-scrapy-cookiecuter

    Run:
        scrapy crawl example
    """

    name = "example"

    def start_requests(self):
        yield scrapy.Request(
            url="http://www.example.com/",
            meta={
                "pod.addr": "http://127.0.0.1:6789/api/queue/batchEnqueue/",
                'depth': 0,
          'download_latency': 0.11835575103759766,
          'download_slot': 'www.gushiwen.cn',
          'download_timeout': 180.0,
          'extractor.depth_limit': 3,
          'extractor.links': [['https://www.gushiwen.cn/default_2.aspx'],
                              ['https://so.gushiwen.cn/shiwenv_fce4c12720ae.aspx',
                               'https://so.gushiwen.cn/shiwenv_c975f408ec63.aspx',
                               'https://so.gushiwen.cn/shiwenv_0927b657115d.aspx',
                               'https://so.gushiwen.cn/shiwenv_0f0d318eeb3a.aspx',
                               'https://so.gushiwen.cn/shiwenv_bfdc62faea89.aspx',
                               'https://so.gushiwen.cn/shiwenv_fc920d0cfe8a.aspx',
                               'https://so.gushiwen.cn/shiwenv_54cbdd9db291.aspx',
                               'https://so.gushiwen.cn/shiwenv_f3d6556dbcad.aspx',
                               'https://so.gushiwen.cn/shiwenv_ae3911122348.aspx',
                               'https://so.gushiwen.cn/shiwenv_a17db42e5e8d.aspx']],
          'extractor.rules': [{'allow': [],
                               'allow_domains': [],
                               'deny': [],
                               'deny_domains': [],
                               'id': '36b763c3-c189-4070-9fe4-917dd7331bf8',
                               'meta': {
                                        "pod.addr": "http://127.0.0.1:6789/api/queue/batchEnqueue/",
                                        'named.group': '古诗文',
                                        'named.tag': '列表页'},
                               'restrict_css': ['.pagesright > .amore'],
                               'restrict_xpaths': [],
                               'type': 'lxml'},
                              {'allow': [],
                               'allow_domains': [],
                               'deny': [],
                               'deny_domains': [],
                               'id': 'ad1b0773-c6bb-4f39-92f7-e30ef0a43815',
                               'meta': {
                                        "pod.addr": "http://127.0.0.1:6789/api/queue/batchEnqueue/",
                                        'named.group': '古诗文',
                                        'named.tag': '列表页'},
                               'restrict_css': ['.sons > .cont > '
                                                'p:first-of-type > a'],
                               'restrict_xpaths': [],
                               'type': 'lxml'}]},
        )

    def parse(self, response):
        response = self.lx_manager.process_response(response)
        yield fetch_record(response=response)

    def _set_lxmanager(self, crawler):
        self.lx_manager = LxExtensionManager.from_crawler(crawler)

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        spider._set_lxmanager(crawler)
        return spider
