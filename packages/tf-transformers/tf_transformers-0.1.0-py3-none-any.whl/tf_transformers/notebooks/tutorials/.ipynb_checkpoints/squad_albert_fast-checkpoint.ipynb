{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tf_transformers.utils.tokenization import BasicTokenizer\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.data.squad_utils import (\n",
    "    read_squad_examples,\n",
    "    post_clean_train_squad,\n",
    "    example_to_features_using_fast_sp_alignment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "basic_tokenizer = BasicTokenizer(do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_PEICE = 'Ä '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Time taken 0.08691596984863281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.7810819149017334\n",
      "time taken 1.1453638076782227 seconds\n"
     ]
    }
   ],
   "source": [
    "input_file_path = '/Users/PRVATE/official_datasets/squad_v1/train-v1.1.json'\n",
    "is_training = True\n",
    "\n",
    "start_time = time.time()\n",
    "train_examples = read_squad_examples(\n",
    "      input_file=input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False,\n",
    "      translated_input_folder=None)\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "\n",
    "# Postprocess (clean text to avoid some unwanted unicode charcaters)\n",
    "train_examples_updated, failed_examples = post_clean_train_squad(train_examples[:100], basic_tokenizer)\n",
    "\n",
    "\n",
    "# Convert question, context and answer to proper features (tokenized words)\n",
    "all_features = []\n",
    "# This is a generator\n",
    "for feature in example_to_features_using_fast_sp_alignment(tokenizer, train_examples_updated, is_training, max_seq_length = 384, \n",
    "                                                           max_query_length=64, doc_stride=128, SPECIAL_PIECE=SPECIAL_PEICE):\n",
    "    all_features.append(feature)\n",
    "end_time = time.time()\n",
    "print(\"time taken {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 100\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to id and add type_ids\n",
    "# input_mask etc\n",
    "# This is user specific/ tokenizer specific\n",
    "# Eg: Roberta has input_type_ids = 0, BERT has input_type_ids = [0, 1]\n",
    "\n",
    "def process_features():\n",
    "    result = {}\n",
    "    for f in all_features:\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(f['input_ids'])\n",
    "        input_type_ids = tf.zeros_like(input_ids).numpy().tolist()\n",
    "        input_mask = tf.ones_like(input_ids).numpy().tolist()\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['start_position'] = f['start_position']\n",
    "        result['end_position']   = f['end_position']\n",
    "        yield result\n",
    "        \n",
    "\n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "from tf_transformers.data import TFWriter\n",
    "\n",
    "schema = {'input_ids': (\"var_len\", \"int\"), \n",
    "         'input_type_ids': (\"var_len\", \"int\"), \n",
    "         'input_mask': (\"var_len\", \"int\"), \n",
    "         'start_position': (\"var_len\", \"int\"), \n",
    "         'end_position': (\"var_len\", \"int\")}\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name='squad_tfrecord', \n",
    "                    model_dir='squad_tfrecord_roberta',\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=process_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "from tf_transformers.data import TFReader\n",
    "import glob\n",
    "all_files = glob.glob(\"squad_tfrecord_roberta/*.tfrecord\")\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['start_position', 'end_position']\n",
    "tf_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                  batch_size=5, \n",
    "                                  x_keys = x_keys, \n",
    "                                  y_keys = y_keys,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(5, 338), dtype=int32, numpy=\n",
      "array([[    0, 10777,   141, ...,     0,     0,     0],\n",
      "       [    0,  1121, 35284, ...,     0,     0,     0],\n",
      "       [    0,  6179,   171, ...,  7077,     4,     2],\n",
      "       [    0,   970,    32, ...,     0,     0,     0],\n",
      "       [    0,  6179,   171, ...,  7077,     4,     2]], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(5, 338), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>, 'input_type_ids': <tf.Tensor: shape=(5, 338), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>} {'end_position': <tf.Tensor: shape=(5, 1), dtype=int32, numpy=\n",
      "array([[58],\n",
      "       [21],\n",
      "       [25],\n",
      "       [88],\n",
      "       [31]], dtype=int32)>, 'start_position': <tf.Tensor: shape=(5, 1), dtype=int32, numpy=\n",
      "array([[58],\n",
      "       [19],\n",
      "       [23],\n",
      "       [88],\n",
      "       [29]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for sample_inputs, sample_labels in tf_dataset:\n",
    "    print(sample_inputs, sample_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "from tf_transformers.models import RobertaModel\n",
    "model_layer, model, config = RobertaModel(model_name='roberta-base')\n",
    "model.load_checkpoint(\"/Users/PRVATE/tf_transformers_models/roberta-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.core import LegacyModel, LegacyLayer\n",
    "\n",
    "\n",
    "class Span_Selection(LegacyLayer):\n",
    "    def __init__(self, model, use_all_layers=False, activation=\"tanh\", **kwargs):\n",
    "        super(Span_Selection, self).__init__(**kwargs)\n",
    "        self.model = model\n",
    "        if isinstance(model, LegacyModel):\n",
    "            self.model_config = model.model_config\n",
    "        elif isinstance(model, tf.keras.layers.Layer):\n",
    "            self.model_config = model._config_dict\n",
    "        self.use_all_layers = use_all_layers\n",
    "        self.logits_layer = tf.keras.layers.Dense(\n",
    "            2,\n",
    "            activation=activation,\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"glorot_uniform\",\n",
    "            bias_initializer=\"zeros\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        result = self.model(inputs)\n",
    "        start_logits_outputs = []\n",
    "        end_logits_outputs   = []\n",
    "        if self.use_all_layers:\n",
    "            # each layer token embeddings\n",
    "            for token_embeddings in result[\"all_layer_token_embeddings\"]:\n",
    "                outputs = self.logits_layer(token_embeddings)\n",
    "                start_logits = outputs[:, :, 0]\n",
    "                end_logits = outputs[:, :, 1]\n",
    "                start_logits_outputs.append(start_logits)\n",
    "                end_logits_outputs.append(end_logits)\n",
    "            return {'start_logits': start_logits_outputs, 'end_logits': end_logits_outputs}\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # last layer token embeddings\n",
    "            token_embeddings = result[\"token_embeddings\"]\n",
    "            outputs = self.logits_layer(token_embeddings)\n",
    "            start_logits = outputs[:, :, 0]\n",
    "            end_logits = outputs[:, :, 1]\n",
    "            return {\n",
    "                    \"start_logits\": start_logits,\n",
    "                    \"end_logits\": end_logits,\n",
    "            }\n",
    "        \n",
    "    def get_model(self):\n",
    "        layer_output = self(self.model.input)\n",
    "        model = LegacyModel(inputs=self.model.input, outputs=layer_output, name='span_selection')\n",
    "        model.model_config = self.model_config\n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "span_selection_layer = Span_Selection(model=model,\n",
    "                                      use_all_layers=True, \n",
    "                                      is_training=True)\n",
    "span_selection_model = span_selection_layer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_loss(position, logits):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.squeeze(position)))\n",
    "    return loss\n",
    "\n",
    "    \n",
    "def start_span_loss_all_layers(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    layer_loss = []\n",
    "    model_outputs = y_pred_dict['start_logits']\n",
    "    for start_logits in model_outputs:\n",
    "        loss = span_loss(y_true_dict['start_position'], start_logits)\n",
    "        layer_loss.append(loss)\n",
    "    return tf.reduce_mean(layer_loss)\n",
    "\n",
    "def end_span_loss_all_layers(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    layer_loss = []\n",
    "    model_outputs = y_pred_dict['end_logits']\n",
    "    for end_logits in model_outputs:\n",
    "        loss = span_loss(y_true_dict['end_position'], end_logits)\n",
    "        layer_loss.append(loss)\n",
    "    return tf.reduce_mean(layer_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = span_selection_model(sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = {'start_logits': start_span_loss_all_layers, \n",
    "           'end_logits': end_span_loss_all_layers}\n",
    "span_selection_model.compile2(optimizer=tf.keras.optimizers.Adam(), \n",
    "                            loss=None, \n",
    "                            custom_loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 43s 4s/step - loss: 10.8664 - end_logits_loss: 5.4724 - start_logits_loss: 5.3941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15ad95f70>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_selection_model.fit(tf_dataset, epochs=1, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
