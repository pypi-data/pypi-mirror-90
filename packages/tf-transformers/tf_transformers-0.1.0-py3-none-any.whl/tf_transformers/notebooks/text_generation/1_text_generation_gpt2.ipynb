{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/PRVATE/Documents/tf_transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import time\n",
    "import tensorflow_hub as hub\n",
    "from tf_transformers.tokenizer import GPT2Tokenizer\n",
    "from tf_transformers.models import GPT2Encoder\n",
    "from tf_transformers.data import pad_dataset\n",
    "\n",
    "from tf_transformers.text import TextDecoder\n",
    "\n",
    "from pprint import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tokenizer is not necessary\n",
    "# We can use amazing HuggingFace tokenizer library also\n",
    "\n",
    "# GPT2 Tokenizer\n",
    "tokenizer_location = \"/Users/PRVATE/HUggingFace_Models/tokenizer_vocab_models/\"\n",
    "\n",
    "merges_file_path = os.path.join(tokenizer_location, \"gpt2/merges.txt\")\n",
    "vocab_path = os.path.join(tokenizer_location, \"gpt2/vocab.json\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    merges_file_path=merges_file_path, vocab_path=vocab_path, do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "module_output_path = \"model_pb/\"\n",
    "loaded = tf.saved_model.load(module_output_path)\n",
    "gpt2_model_pb = loaded.signatures[\"serving_default\"]\n",
    "\n",
    "\n",
    "# Load saved model using Hub kerasLyaer\n",
    "module_output_path = \"model_pb/\"\n",
    "gpt2_model_hub = hub.KerasLayer(\n",
    "    module_output_path,\n",
    "    signature=\"serving_default\",\n",
    "    trainable=False,\n",
    "    signature_outputs_as_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to tokens (for GPT2 Model)\n",
    "@pad_dataset\n",
    "def tokenizer_fn(tokenizer, text_list):\n",
    "    \"\"\"Tokenizer fn should return a dict (no padding is required).\n",
    "    Make sure, you pass all primary keys required to the model\n",
    "\n",
    "    text_list: a list of text\n",
    "\n",
    "    {'input_ids': tf.constant([[1, 2]]),\n",
    "     'input_mask': tf.constant([[1, 1]]),\n",
    "     'input_type_ids': tf.constant([[1, 0]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    input_type_ids = []\n",
    "    for text in text_list:\n",
    "        input_ids.append(tokenizer.encode(text))\n",
    "    inputs = {\"input_ids\": input_ids}\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# # Decoder using saved_model\n",
    "\n",
    "decoder = TextDecoder(\n",
    "    model=gpt2_model_pb, hidden_dimension=768, num_attention_heads=12, num_layers=12\n",
    ")\n",
    "\n",
    "# # Decoder using hub\n",
    "\n",
    "decoder_hub = TextDecoder(\n",
    "    model=gpt2_model_hub, hidden_dimension=768, num_attention_heads=12, num_layers=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"Sachin Tendulkar is one of the finest\", \"I like to walk with my dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy\n",
    "start_time = time.time()\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "result = decoder.decode(inputs, max_iterations=25, mode=\"greedy\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "\n",
    "print(\"_______________________________________________________\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy with EOS_ID\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "result = decoder.decode(inputs, max_iterations=25, mode=\"greedy\", eos_id=13)\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print(\"_______________________________________________________\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy using Hub\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "result = decoder_hub.decode(inputs, max_iterations=25, mode=\"greedy\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print(\"_______________________________________________________\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy using Hub EOS ID\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "result = decoder_hub.decode(inputs, max_iterations=25, mode=\"greedy\", eos_id=13)\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print(\"_______________________________________________________\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "result = decoder.decode(\n",
    "    inputs, max_iterations=25, beam_size=5, mode=\"beam\", do_sample=False, eos_id=-100\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print(\"_______________________________________________________\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search EOS ID\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "result = decoder.decode(\n",
    "    inputs, max_iterations=25, beam_size=5, mode=\"beam\", do_sample=False, eos_id=13\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print(\"_______________________________________________________\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search using Hub\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "result = decoder_hub.decode(\n",
    "    inputs, max_iterations=25, beam_size=5, mode=\"beam\", do_sample=False, eos_id=-100\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print(\"_______________________________________________________\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search Hub with EOS ID\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "result = decoder_hub.decode(\n",
    "    inputs, max_iterations=25, beam_size=5, mode=\"beam\", do_sample=False, eos_id=13\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print(\"_______________________________________________________\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top K Sampling , with Top P ( Nucleus )\n",
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer_fn(tokenizer, text_list)\n",
    "result = decoder.decode(\n",
    "    inputs,\n",
    "    max_iterations=25,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    top_p=0.75,\n",
    "    num_return_sequences=3,\n",
    "    mode=\"top_k_top_p\",\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print(\"_______________________________________________________\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf model_ckpt/\n",
    "!rm -rf model_pb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
