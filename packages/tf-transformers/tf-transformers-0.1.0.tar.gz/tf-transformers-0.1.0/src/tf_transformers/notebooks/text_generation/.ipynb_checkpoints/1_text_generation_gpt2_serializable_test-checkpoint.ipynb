{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/PRVATE/Documents/tf_transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from transformers import TFGPT2Model\n",
    "from tf_transformers.models import GPT2Encoder\n",
    "\n",
    "from tf_transformers.core import LegacyModule\n",
    "from tf_transformers.tokenizer import GPT2Tokenizer\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at /Users/PRVATE/HUggingFace_Models/gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load HF model\n",
    "\n",
    "# Always do this\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "local_dir = \"/Users/PRVATE/HUggingFace_Models/\"\n",
    "hf_model_name = \"gpt2\"\n",
    "if local_dir:\n",
    "    hf_model_location = local_dir + hf_model_name\n",
    "\n",
    "model_hf = TFGPT2Model.from_pretrained(hf_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:past_length ---> Tensor(\"past_length:0\", shape=(1, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key_1:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value_1:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:past_length ---> Tensor(\"past_length_1:0\", shape=(1, None), dtype=int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.GPT2Encoder object at 0x14a46e5b0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x14ac41ee0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.GPT2Encoder object at 0x14a46e5b0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x14ac41ee0>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x14aa712b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "config_location = \"../../configs/model_configs/\" + \"gpt2_base/\" + \"gpt2_config.json\"\n",
    "config = json.load(open(config_location))\n",
    "\n",
    "# Always do this\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# tf_transformers Layer (an extension of Keras Layer)\n",
    "# This is not Keras model, but extension of keras Layer\n",
    "\n",
    "# Save as saved_model\n",
    "# If you want to use the model for Auto Regressive tasks ( text-generation ),\n",
    "# you have to enable pipeline_mode='auto-regressive'.\n",
    "# Because TF needs extra cache inputs in the saved_model format for doing efficient caching\n",
    "\n",
    "model_layer = GPT2Encoder(\n",
    "    config=config,\n",
    "    name=\"gpt2\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    "    pipeline_mode=\"auto-regressive\",\n",
    ")\n",
    "\n",
    "# Convert to tf.keras.Model\n",
    "model_tf_transformers = model_layer.get_and_load_model(model_dir=None)\n",
    "\n",
    "# And now load the checkpints from previously saved model\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model_tf_transformers)\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=\"model_ckpt\", max_to_keep=1)\n",
    "status = checkpoint.restore(manager.latest_checkpoint)\n",
    "\n",
    "# Important\n",
    "status.assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Succesfully loaded reserved token vocab\n"
     ]
    }
   ],
   "source": [
    "# This tokenizer is not necessary\n",
    "# We can use amazing HuggingFace tokenizer library also\n",
    "\n",
    "merges_file_path = \"../../tokenizer/tokenizer_vocab_models/gpt2/merges.txt\"\n",
    "vocab_path = \"../../tokenizer/tokenizer_vocab_models/gpt2/vocab.json\"\n",
    "reserved_vocab_path = (\n",
    "    \"../../tokenizer/tokenizer_vocab_models/gpt2/reserved_tokens_map.json\"\n",
    ")\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    merges_file_path=merges_file_path,\n",
    "    vocab_path=vocab_path,\n",
    "    reserved_tokens_map_file=reserved_vocab_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_fn(text_list):\n",
    "    \"\"\"Tokenizer fn should return a dict (no padding is required).\n",
    "    Make sure, you pass all primary keys required to the model\n",
    "\n",
    "    text_list: a list of text\n",
    "\n",
    "    {'input_ids': tf.constant([[1, 2]]),\n",
    "     'input_mask': tf.constant([[1, 1]]),\n",
    "     'input_type_ids': tf.constant([[1, 0]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    for text in text_list:\n",
    "        input_ids.append(tokenizer.encode(text)[1])\n",
    "    inputs = {\"input_ids\": input_ids}\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.text import TextDecoder\n",
    "from tf_transformers.text import TextDecoderSerializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer_beam = TextDecoder(\n",
    "    tokenizer_fn=tokenizer_fn,\n",
    "    model=model_tf_transformers,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    attention_state=64,\n",
    "    input_mask_ids=None,\n",
    "    input_type_ids=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"Sachin Tendulkar is one of the finest\", \n",
    "            \"I like to walk with my dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 3.284972667694092 seconds\n",
      "_______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Beam Search\n",
    "\n",
    "start_time = time.time()\n",
    "result_beam = decoder_layer_beam.decode(\n",
    "    text_list, max_iterations=25, beam_size=2, mode=\"beam\", do_sample=False, eos_id=None\n",
    ")\n",
    "# for i in range(len(result[\"input_ids\"])):\n",
    "#     for beam_predicted_ids in result[\"predicted_ids\"][i]:\n",
    "#         print(\n",
    "#             tokenizer.decode(\n",
    "#                 tf.concat([result_beam[\"input_ids\"][i], beam_predicted_ids], axis=0).numpy()\n",
    "#             )\n",
    "#         )\n",
    "#         print(\"--------------\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print('_______________________________________________________')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 2.6476550102233887 seconds\n",
      "_______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "\n",
    "start_time = time.time()\n",
    "result_greedy = decoder_layer_beam.decode(\n",
    "    text_list, max_iterations=25, mode=\"greedy\", do_sample=False, eos_id=None\n",
    ")\n",
    "# for i in range(len(result[\"input_ids\"])):\n",
    "#     for beam_predicted_ids in result[\"predicted_ids\"][i]:\n",
    "#         print(\n",
    "#             tokenizer.decode(\n",
    "#                 tf.concat([result_beam[\"input_ids\"][i], beam_predicted_ids], axis=0).numpy()\n",
    "#             )\n",
    "#         )\n",
    "#         print(\"--------------\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print('_______________________________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iterations': 24,\n",
       " 'input_ids': [[50, 620, 259, 48664, 12171, 283, 318, 530, 286, 262, 18822],\n",
       "  [40, 588, 284, 2513, 351, 616, 3290]],\n",
       " 'predicted_ids': <tf.Tensor: shape=(2, 1, 25), dtype=int32, numpy=\n",
       " array([[[1938,  287,  262,  995,   13,  679,  318,  257,  845,  922,\n",
       "          2137,   11,  475,  339,  318,  407,  257, 1049, 2137,   13,\n",
       "           679,  318,  257,  845,  922]],\n",
       " \n",
       "        [[  11,  475,  314,  836,  470,  588,  284, 2513,  351,  616,\n",
       "          3290,   13,  314,  588,  284, 2513,  351,  616, 3290,   11,\n",
       "           475,  314,  836,  470,  588]]], dtype=int32)>,\n",
       " 'matched_eos_pos': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([-1, -1], dtype=int32)>}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 3.1377787590026855 seconds\n",
      "_______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Top K top P Search\n",
    "\n",
    "start_time = time.time()\n",
    "result_top_k_top_p = decoder_layer_beam.decode(\n",
    "    text_list, max_iterations=25, mode=\"top_k_top_p\", top_k=50, top_p=0.7, do_sample=False, eos_id=None, \n",
    "    num_return_sequences=2\n",
    ")\n",
    "# for i in range(len(result[\"input_ids\"])):\n",
    "#     for beam_predicted_ids in result[\"predicted_ids\"][i]:\n",
    "#         print(\n",
    "#             tokenizer.decode(\n",
    "#                 tf.concat([result_beam[\"input_ids\"][i], beam_predicted_ids], axis=0).numpy()\n",
    "#             )\n",
    "#         )\n",
    "#         print(\"--------------\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print('_______________________________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iterations': 24,\n",
       " 'input_ids': [[50, 620, 259, 48664, 12171, 283, 318, 530, 286, 262, 18822],\n",
       "  [40, 588, 284, 2513, 351, 616, 3290]],\n",
       " 'predicted_ids': <tf.Tensor: shape=(2, 2, 25), dtype=int32, numpy=\n",
       " array([[[1938,  287,  262,  995,   13,  679,  318,  257,  845,  922,\n",
       "          2137,   11,  475,  339,  318,  407,  257, 1049, 2137,   13,\n",
       "           679,  318,  257,  845,  922],\n",
       "         [1938,  287,  262,  995,   13,  679,  318,  257,  845,  922,\n",
       "          2137,   11,  475,  339,  318,  407,  257, 1049, 2137,   13,\n",
       "           679,  318,  257,  845,  922]],\n",
       " \n",
       "        [[  11,  475,  314,  836,  470,  588,  284, 2513,  351,  616,\n",
       "          3290,   13,  314,  588,  284, 2513,  351,  616, 3290,   11,\n",
       "           475,  314,  836,  470,  588],\n",
       "         [  11,  475,  314,  836,  470,  588,  284, 2513,  351,  616,\n",
       "          3290,   13,  314,  588,  284, 2513,  351,  616, 3290,   11,\n",
       "           475,  314,  836,  470,  588]]], dtype=int32)>,\n",
       " 'matched_eos_pos': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -1, -1, -1], dtype=int32)>}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_top_k_top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 2.552328109741211 seconds\n"
     ]
    }
   ],
   "source": [
    "decoder_layer_serializable = TextDecoderSerializable(\n",
    "    model_tf_transformers,\n",
    "    input_name_list=[\"input_ids\"],\n",
    "    max_iterations=25,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    attention_state=64,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    "    input_mask_ids=None,\n",
    "    input_type_ids=None,\n",
    ")\n",
    "\n",
    "inputs_for_serializable = tokenizer_fn(text_list)\n",
    "\n",
    "\n",
    "inputs_for_serializable[\"input_ids\"] = tf.ragged.constant(inputs_for_serializable[\"input_ids\"]).to_tensor(-1)\n",
    "start_time = time.time()\n",
    "results_serializable_greedy = decoder_layer_serializable(inputs_for_serializable)\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 3.2864420413970947 seconds\n"
     ]
    }
   ],
   "source": [
    "decoder_layer_serializable = TextDecoderSerializable(\n",
    "    model_tf_transformers,\n",
    "    input_name_list=[\"input_ids\"],\n",
    "    max_iterations=25,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    attention_state=64,\n",
    "    mode=\"beam\",\n",
    "    do_sample=False,\n",
    "    beam_size=2,\n",
    "    eos_id=-100,\n",
    "    input_mask_ids=None,\n",
    "    input_type_ids=None,\n",
    ")\n",
    "\n",
    "inputs_for_serializable = tokenizer_fn(text_list)\n",
    "inputs_for_serializable[\"input_ids\"] = tf.ragged.constant(inputs_for_serializable[\"input_ids\"]).to_tensor(-1)\n",
    "start_time = time.time()\n",
    "results_serializable_beam = decoder_layer_serializable(inputs_for_serializable)\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 3.2622859477996826 seconds\n"
     ]
    }
   ],
   "source": [
    "decoder_layer_serializable = TextDecoderSerializable(\n",
    "    model_tf_transformers,\n",
    "    input_name_list=[\"input_ids\"],\n",
    "    max_iterations=25,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    attention_state=64,\n",
    "    mode=\"top_k_top_p\",\n",
    "    do_sample=False,\n",
    "    num_return_sequences=2,\n",
    "    top_k = 50,\n",
    "    top_p = 0.7,\n",
    "    eos_id=-100,\n",
    "    input_mask_ids=None,\n",
    "    input_type_ids=None,\n",
    ")\n",
    "\n",
    "inputs_for_serializable = tokenizer_fn(text_list)\n",
    "inputs_for_serializable[\"input_ids\"] = tf.ragged.constant(inputs_for_serializable[\"input_ids\"]).to_tensor(-1)\n",
    "start_time = time.time()\n",
    "results_serializable_top_k_top_p = decoder_layer_serializable(inputs_for_serializable)\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.assert_equal(result_greedy['predicted_ids'], results_serializable_greedy['predicted_ids'])\n",
    "tf.assert_equal(result_beam['predicted_ids'], results_serializable_beam['predicted_ids'])\n",
    "tf.assert_equal(results_serializable_top_k_top_p['predicted_ids'], result_top_k_top_p['predicted_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
