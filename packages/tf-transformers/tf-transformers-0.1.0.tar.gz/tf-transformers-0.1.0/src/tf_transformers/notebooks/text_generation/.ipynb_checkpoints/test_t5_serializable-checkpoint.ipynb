{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/PRVATE/Documents/tf_transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFT5Model\n",
    "from tf_transformers.models import T5Encoder\n",
    "from tf_transformers.models import EncoderDecoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from tf_transformers.core import LegacyModule\n",
    "from tf_transformers.utils import convert_t5_hf_to_tf_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to                     True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n",
      "INFO:absl:decoder_encoder_mask ---> Tensor(\"decoder_encoder_mask:0\", shape=(None, None, None), dtype=float32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 8, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "config_location = \"../../configs/model_configs/\" + \"t5_small/\" + \"t5_config.json\"\n",
    "\n",
    "# Lets load pure Auto regressive caching mode\n",
    "\n",
    "config = json.load(open(config_location))\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Encoder layer (Nothing to cache)\n",
    "config[\"mask_mode\"] = \"user_defined\"\n",
    "encoder_layer = T5Encoder(\n",
    "    config=config, mask_mode=config[\"mask_mode\"], is_training=False, name=\"t5_encoder\"\n",
    ")\n",
    "\n",
    "# Decoder\n",
    "# Set pipeline_mode = 'auto-regressive'\n",
    "# Only by that, we can enable caching in decoder side\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "config[\"bidirectional\"] = False\n",
    "config[\"mask_mode\"] = \"causal\"\n",
    "decoder_layer = T5Encoder(\n",
    "    config=config,\n",
    "    name=\"t5_decoder\",\n",
    "    mask_mode=\"causal\",\n",
    "    is_decoder=True,\n",
    "    is_training=False,\n",
    "    use_dropout=False,\n",
    "    pipeline_mode=\"auto-regressive\",\n",
    "    share_encoder_embeddings=True,\n",
    "    encoder_embedding_layer=encoder_layer._embedding_layer,\n",
    ")\n",
    "\n",
    "\n",
    "# Train mode\n",
    "enc_dec_model = EncoderDecoder(\n",
    "    encoder=encoder_layer,\n",
    "    decoder=decoder_layer,\n",
    "    is_training=False,\n",
    "    name=\"t5_small\",\n",
    "    use_dropout=False,\n",
    ")\n",
    "enc_dec_model = enc_dec_model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.EncoderDecoder object at 0x1644da490> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x165131340>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.EncoderDecoder object at 0x1644da490> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x165131340>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x16531f160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now load the checkpints from previously saved model\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=enc_dec_model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=\"model_ckpt\", max_to_keep=1)\n",
    "status = checkpoint.restore(manager.latest_checkpoint)\n",
    "\n",
    "# Important\n",
    "status.assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_inputs = {'encoder_input_ids': tf.constant([[13959,\n",
    "   1566,\n",
    "   12,\n",
    "   5093,\n",
    "   10,\n",
    "   27,\n",
    "   333,\n",
    "   25,\n",
    "   11,\n",
    "   27,\n",
    "   241,\n",
    "   12,\n",
    "   608,\n",
    "   3,\n",
    "   9,\n",
    "   7966,\n",
    "   484,\n",
    "   1]]),\n",
    " 'encoder_input_mask': tf.constant([[1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1,\n",
    "   1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Greedy\n",
    "# decoder_layer_serializable = TextDecoderSerializableSeq2Seq(\n",
    "#     enc_dec_model,\n",
    "#     max_iterations=25,\n",
    "#     decode_start_token_id=0,\n",
    "#     encoder_hidden_size=512,\n",
    "#     decoder_hidden_size = 512,\n",
    "#     decoder_num_layers = 6,\n",
    "#     decoder_num_attention_heads=8,\n",
    "#     mode=\"greedy\",\n",
    "#     do_sample=False,\n",
    "#     eos_id=-100,\n",
    "#     decoder_input_type_ids=0,\n",
    "# )\n",
    "\n",
    "# decoder_model  = decoder_layer_serializable.get_model()\n",
    "# decoder_module = LegacyModule(decoder_model)\n",
    "# decoder_module.save(\"model_temp_pb\")\n",
    "\n",
    "# loaded = tf.saved_model.load(\"model_temp_pb/\")\n",
    "# model_pb = loaded.signatures['serving_default']\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# result = model_pb(**main_inputs)\n",
    "# end_time = time.time()\n",
    "# print(\"Time taken {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = decoder_layer_serializable(main_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Beam\n",
    "# decoder_layer_serializable = TextDecoderSerializableSeq2Seq(\n",
    "#     enc_dec_model,\n",
    "#     max_iterations=25,\n",
    "#     decode_start_token_id=0,\n",
    "#     encoder_hidden_size=512,\n",
    "#     decoder_hidden_size = 512,\n",
    "#     decoder_num_layers = 6,\n",
    "#     decoder_num_attention_heads=8,\n",
    "#     beam_size=2,\n",
    "#     mode=\"beam\",\n",
    "#     do_sample=False,\n",
    "#     eos_id=-100,\n",
    "#     decoder_input_type_ids=0,\n",
    "# )\n",
    "\n",
    "# decoder_model  = decoder_layer_serializable.get_model()\n",
    "# decoder_module = LegacyModule(decoder_model)\n",
    "# decoder_module.save(\"model_temp_pb\")\n",
    "\n",
    "# loaded = tf.saved_model.load(\"model_temp_pb/\")\n",
    "# model_pb = loaded.signatures['serving_default']\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# result = model_pb(**main_inputs)\n",
    "# end_time = time.time()\n",
    "# print(\"Time taken {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_temp_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_temp_pb/assets\n"
     ]
    }
   ],
   "source": [
    "# Top K Top P\n",
    "decoder_layer_serializable = TextDecoderSerializableSeq2Seq(\n",
    "    enc_dec_model,\n",
    "    max_iterations=25,\n",
    "    decode_start_token_id=0,\n",
    "    encoder_hidden_size=512,\n",
    "    decoder_hidden_size = 512,\n",
    "    decoder_num_layers = 6,\n",
    "    decoder_num_attention_heads=8,\n",
    "    beam_size=2,\n",
    "    mode=\"top_k_top_p\",\n",
    "    top_k = 10,\n",
    "    top_p = 0.75,\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    "    decoder_input_type_ids=0,\n",
    "    num_return_sequences = 2\n",
    ")\n",
    "\n",
    "decoder_model  = decoder_layer_serializable.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(\"model_temp_pb\")\n",
    "\n",
    "# loaded = tf.saved_model.load(\"model_temp_pb/\")\n",
    "# model_pb = loaded.signatures['serving_default']\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# result = model_pb(**main_inputs)\n",
    "# end_time = time.time()\n",
    "# print(\"Time taken {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(\"model_temp_pb/\")\n",
    "model_pb = loaded.signatures['serving_default']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.6257359981536865 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "result = model_pb(**main_inputs)\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
