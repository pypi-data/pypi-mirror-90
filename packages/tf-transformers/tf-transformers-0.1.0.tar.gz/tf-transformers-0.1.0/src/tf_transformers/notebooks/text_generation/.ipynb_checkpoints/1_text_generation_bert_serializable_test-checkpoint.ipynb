{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/PRVATE/Documents/tf_transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from transformers import TFAlbertModel\n",
    "from tf_transformers.models import AlbertEncoder\n",
    "\n",
    "from tf_transformers.core import LegacyModule\n",
    "from transformers import AlbertTokenizer\n",
    "\n",
    "from tf_transformers.utils import convert_albert_hf_to_tf_transformers\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFAlbertModel.\n",
      "\n",
      "All the layers of TFAlbertModel were initialized from the model checkpoint at /Users/PRVATE/HUggingFace_Models/albert-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load HF model\n",
    "\n",
    "# Always do this\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "local_dir = \"/Users/PRVATE/HUggingFace_Models/\"\n",
    "hf_model_name = \"albert-base-v2\"\n",
    "if local_dir:\n",
    "    hf_model_location = local_dir + hf_model_name\n",
    "\n",
    "model_hf = TFAlbertModel.from_pretrained(hf_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to                     True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Deleteing huggingface model for saving memory\n",
      "INFO:absl:Done assigning variables weights. Total 25\n"
     ]
    }
   ],
   "source": [
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "config_location = (\n",
    "    \"../../configs/model_configs/\" + \"albert_base_v2/\" + \"albert_config.json\"\n",
    ")\n",
    "config = json.load(open(config_location))\n",
    "\n",
    "# Always do this\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# tf_transformers Layer (an extension of Keras Layer)\n",
    "# This is not Keras model, but extension of keras Layer\n",
    "\n",
    "# Save as saved_model\n",
    "# If you want to use the model for Auto Regressive tasks ( text-generation ),\n",
    "# you have to enable pipeline_mode='auto-regressive'.\n",
    "# Because TF needs extra cache inputs in the saved_model format for doing efficient caching\n",
    "\n",
    "model_layer = AlbertEncoder(\n",
    "    config=config,\n",
    "    name=\"albert\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    "    use_dropout=False,\n",
    ")\n",
    "\n",
    "# Convert to tf.keras.Model\n",
    "model_tf_transformers = model_layer.get_and_load_model(model_dir=None)\n",
    "convert_albert_hf_to_tf_transformers(model_hf, model_tf_transformers, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved at model_albert_ckpt/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# If you want to save the model as checkpoints\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model_tf_transformers)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=\"model_albert_ckpt\", max_to_keep=1\n",
    ")\n",
    "manager.save()\n",
    "print(\"Saved at {}\".format(manager.latest_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_transformers.core.legacy_model.LegacyModel at 0x14f3ca5b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_output --> tf.Tensor(12.337963, shape=(), dtype=float32) --> (2, 768)\n",
      "token_embeddings --> tf.Tensor(-193.53201, shape=(), dtype=float32) --> (2, 5, 768)\n",
      "token_logits --> tf.Tensor(-18578.355, shape=(), dtype=float32) --> (2, 5, 30000)\n",
      "last_token_logits --> tf.Tensor(-3803.8923, shape=(), dtype=float32) --> (2, 30000)\n"
     ]
    }
   ],
   "source": [
    "# Please have a look at tf_transformers/extra/*.py for reference values\n",
    "\n",
    "input_ids = tf.constant([[1, 9, 10, 11, 23], [1, 22, 234, 432, 2349]])\n",
    "input_mask = tf.ones_like(input_ids)\n",
    "input_type_ids = tf.ones_like(input_ids)\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"input_mask\": input_mask,\n",
    "    \"input_type_ids\": input_type_ids,\n",
    "}\n",
    "\n",
    "results_tf_transformers = model_tf_transformers(inputs)\n",
    "for k, r in results_tf_transformers.items():\n",
    "    if isinstance(r, list):\n",
    "        continue\n",
    "    print(k, \"-->\", tf.reduce_sum(r), \"-->\", r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_2:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask_2:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids_2:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:past_length ---> Tensor(\"past_length:0\", shape=(1, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_3:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask_3:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids_3:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:all_cache_key ---> Tensor(\"all_cache_key_1:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:all_cache_value ---> Tensor(\"all_cache_value_1:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:past_length ---> Tensor(\"past_length_1:0\", shape=(1, None), dtype=int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.albert.AlbertEncoder object at 0x14f6c8b80> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x14f7b32e0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.albert.AlbertEncoder object at 0x14f6c8b80> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x14f7b32e0>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1502523a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_layer = AlbertEncoder(\n",
    "    config=config,\n",
    "    name=\"albert\",\n",
    "    mask_mode=config[\"mask_mode\"],\n",
    "    is_training=False,\n",
    "    pipeline_mode=\"auto-regressive\",\n",
    ")\n",
    "\n",
    "# Convert to tf.keras.Model\n",
    "model_tf_transformers = model_layer.get_and_load_model(model_dir=None)\n",
    "\n",
    "# And now load the checkpints from previously saved model\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model_tf_transformers)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=\"model_albert_ckpt\", max_to_keep=1\n",
    ")\n",
    "status = checkpoint.restore(manager.latest_checkpoint)\n",
    "\n",
    "# Important\n",
    "status.assert_existing_objects_matched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# albert_module = LegacyModule(model_tf_transformers)\n",
    "# albert_module.save(\"model_albert_pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tokenizer is not necessary\n",
    "# We can use amazing HuggingFace tokenizer library also\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_fn(text_list):\n",
    "    \"\"\"Tokenizer fn should return a dict (no padding is required).\n",
    "    Make sure, you pass all primary keys required to the model\n",
    "\n",
    "    text_list: a list of text\n",
    "\n",
    "    {'input_ids': tf.constant([[1, 2]]),\n",
    "     'input_mask': tf.constant([[1, 1]]),\n",
    "     'input_type_ids': tf.constant([[1, 0]])}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    for text in text_list:\n",
    "        input_ids.append(tokenizer.encode(text))\n",
    "    input_ids = tf.ragged.constant(input_ids)\n",
    "    inputs = {\"input_ids\": input_ids}\n",
    "    inputs[\"input_mask\"] = tf.ones_like(input_ids).numpy().tolist()\n",
    "    inputs[\"input_type_ids\"] = tf.zeros_like(input_ids).numpy().tolist()\n",
    "    inputs[\"input_ids\"] = input_ids.numpy().tolist()\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.text import TextDecoder\n",
    "from tf_transformers.text import TextDecoderSerializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer_beam = TextDecoder(\n",
    "    tokenizer_fn=tokenizer_fn,\n",
    "    model=model_tf_transformers,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    attention_state=64,\n",
    "    input_mask_ids=1,\n",
    "    input_type_ids=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"Sachin Tendulkar is one of the finest\", \n",
    "            \"I like to walk with my dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 3.2244322299957275 seconds\n",
      "_______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Beam Search\n",
    "\n",
    "start_time = time.time()\n",
    "result_beam = decoder_layer_beam.decode(\n",
    "    text_list, max_iterations=25, beam_size=2, mode=\"beam\", do_sample=False, eos_id=None\n",
    ")\n",
    "# for i in range(len(result[\"input_ids\"])):\n",
    "#     for beam_predicted_ids in result[\"predicted_ids\"][i]:\n",
    "#         print(\n",
    "#             tokenizer.decode(\n",
    "#                 tf.concat([result_beam[\"input_ids\"][i], beam_predicted_ids], axis=0).numpy()\n",
    "#             )\n",
    "#         )\n",
    "#         print(\"--------------\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print('_______________________________________________________')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 2.4923179149627686 seconds\n",
      "_______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "\n",
    "start_time = time.time()\n",
    "result_greedy = decoder_layer_beam.decode(\n",
    "    text_list, max_iterations=25, mode=\"greedy\", do_sample=False, eos_id=None\n",
    ")\n",
    "# for i in range(len(result[\"input_ids\"])):\n",
    "#     for beam_predicted_ids in result[\"predicted_ids\"][i]:\n",
    "#         print(\n",
    "#             tokenizer.decode(\n",
    "#                 tf.concat([result_beam[\"input_ids\"][i], beam_predicted_ids], axis=0).numpy()\n",
    "#             )\n",
    "#         )\n",
    "#         print(\"--------------\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print('_______________________________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 2.9417638778686523 seconds\n",
      "_______________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Top K top P Search\n",
    "\n",
    "start_time = time.time()\n",
    "result_top_k_top_p = decoder_layer_beam.decode(\n",
    "    text_list, max_iterations=25, mode=\"top_k_top_p\", top_k=50, top_p=0.7, do_sample=False, eos_id=None, \n",
    "    num_return_sequences=2\n",
    ")\n",
    "# for i in range(len(result[\"input_ids\"])):\n",
    "#     for beam_predicted_ids in result[\"predicted_ids\"][i]:\n",
    "#         print(\n",
    "#             tokenizer.decode(\n",
    "#                 tf.concat([result_beam[\"input_ids\"][i], beam_predicted_ids], axis=0).numpy()\n",
    "#             )\n",
    "#         )\n",
    "#         print(\"--------------\")\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "print('_______________________________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 2.503310203552246 seconds\n"
     ]
    }
   ],
   "source": [
    "decoder_layer_serializable = TextDecoderSerializable(\n",
    "    model_tf_transformers,\n",
    "    input_name_list=[\"input_ids\", \"input_mask\", \"input_type_ids\"],\n",
    "    max_iterations=25,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    attention_state=64,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    "    input_mask_ids=1,\n",
    "    input_type_ids=0,\n",
    ")\n",
    "\n",
    "inputs_for_serializable = tokenizer_fn(text_list)\n",
    "\n",
    "main_inputs = {}\n",
    "for k, v in inputs_for_serializable.items():\n",
    "    main_inputs[k] = tf.ragged.constant(v)\n",
    "main_inputs['input_ids'] = main_inputs['input_ids'].to_tensor(-1)\n",
    "main_inputs['input_mask'] = main_inputs['input_mask'].to_tensor(0)\n",
    "main_inputs['input_type_ids'] = main_inputs['input_type_ids'].to_tensor(0)\n",
    "\n",
    "start_time = time.time()\n",
    "results_serializable_greedy = decoder_layer_serializable(main_inputs)\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_temp_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_temp_pb/assets\n"
     ]
    }
   ],
   "source": [
    "decoder_model  = decoder_layer_serializable.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(\"model_temp_pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_transformers.text import (assign_zeros_to_K_V,\n",
    "                                        _log_prob_from_logits,\n",
    "                                        _gather_beams,\n",
    "                                        top_k_logits,\n",
    "                                        top_p_logits)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "class TextDecoderSerializable(tf.keras.layers.Layer):\n",
    "    \"\"\"TextDecoderSerializable - This class is responsible for saving the model along with decoding\n",
    "    operation as a saved_model, which makes deployment in production easier.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        max_iterations,\n",
    "        num_attention_heads,\n",
    "        num_layers,\n",
    "        attention_state,\n",
    "        mode,\n",
    "        input_name_list=None,\n",
    "        beam_size = 1,\n",
    "        eos_id=-100,\n",
    "        do_sample=False,\n",
    "        top_k = 0,\n",
    "        top_p = 0,\n",
    "        num_return_sequences = 1,\n",
    "        input_mask_ids = None,\n",
    "        input_type_ids = None,\n",
    "    ):\n",
    "        \"\"\"[Init]\n",
    "\n",
    "        Args:\n",
    "            model ([tf.keras.Model / tf.keras.Layer]): [The model with which decoding\n",
    "            has to be performed]\n",
    "            max_iterations ([int]): [Maximum iterations for decoding]\n",
    "            num_attention_heads ([int]): [Attention heads of model]\n",
    "            num_layers ([int]): [Number of model layers]\n",
    "            attention_state ([int]): [embedding_size//num_attention_heads]\n",
    "            mode ([str]): ['greedy' , 'beam', 'top_k_top_p']\n",
    "            input_name_list ([List of int]): [Names of model inputs like input_ids, input_mask, etc]\n",
    "            beam_size (int, optional): [Number of beam size]. Defaults to 1.\n",
    "            eos_id (int, optional): [end of sentence token id]. Defaults to -100.\n",
    "            do_sample (bool, optional): [Multinomial sampling]. Defaults to False.\n",
    "            top_k (int, optional): [top k]. Defaults to 0.\n",
    "            top_p (int, optional): [top p Nucleus]. Defaults to 0.\n",
    "            input_mask_ids (int, optional): [if your model has this, provide it]. Defaults to None.\n",
    "            input_type_ids (int, optional): [if your model has this, provide it]. Defaults to None.\n",
    "            num_return_sequences: (int): [No of return sequences for topk top beam] Defaults to 1.\n",
    "        \"\"\"\n",
    "\n",
    "        super(TextDecoderSerializable, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_state = attention_state\n",
    "\n",
    "        self.model = model\n",
    "        # self.input_name_list = input_name_list\n",
    "        self.input_name_list, self.model_inputs = self.get_inputs()\n",
    "        self.input_name_map = {i:k for i, k in enumerate(self.input_name_list)}\n",
    "\n",
    "\n",
    "        self.eos_id = eos_id\n",
    "        self.max_iterations = max_iterations\n",
    "        self.mode = mode\n",
    "\n",
    "        # Mask and type ids\n",
    "        self.input_mask_ids = input_mask_ids\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "        self.beam_size = beam_size\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.do_sample = do_sample\n",
    "        self.num_return_sequences = num_return_sequences\n",
    "\n",
    "\n",
    "        if self.mode == 'greedy':\n",
    "            self.decoder_fn = self.greedy()\n",
    "        elif self.mode == 'beam':\n",
    "            self.decoder_fn = self.beam()\n",
    "        elif self.mode == 'top_k_top_p':\n",
    "            self.decoder_fn = self.top_k_top()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_model(self):\n",
    "        # Call the model in init itself\n",
    "        layer_outputs = self(self.model_inputs)\n",
    "        decoder_model = tf.keras.Model(\n",
    "                        inputs=self.model_inputs,\n",
    "                        outputs=layer_outputs,\n",
    "                        name='decoder_model')\n",
    "        return decoder_model\n",
    "\n",
    "\n",
    "    def get_inputs(self):\n",
    "\n",
    "        input_ids = tf.keras.layers.Input(\n",
    "                        shape=(None,), batch_size = None, ragged=False, dtype=tf.int32, name='input_ids')\n",
    "        input_mask = tf.keras.layers.Input(\n",
    "                        shape=(None,), batch_size = None, ragged=False, dtype=tf.int32, name='input_mask')\n",
    "        input_type_ids = tf.keras.layers.Input(\n",
    "                        shape=(None,), batch_size = None, ragged=False, dtype=tf.int32, name='input_type_ids')\n",
    "        self.input_name_list = []\n",
    "        if 'input_ids' in self.model.input:\n",
    "            self.input_name_list.append('input_ids')\n",
    "        if 'input_mask' in self.model.input:\n",
    "            self.input_name_list.append('input_mask')\n",
    "        if 'input_type_ids' in self.model.input:\n",
    "            self.input_name_list.append('input_type_ids')\n",
    "\n",
    "        inputs = {}\n",
    "        for name in self.input_name_list:\n",
    "            if name == 'input_ids':\n",
    "                inputs['input_ids'] = input_ids\n",
    "                continue\n",
    "            if name == 'input_mask':\n",
    "                inputs['input_mask'] = input_mask\n",
    "            if name == 'input_type_ids':\n",
    "                inputs['input_type_ids'] = input_type_ids\n",
    "\n",
    "        return self.input_name_list, inputs\n",
    "\n",
    "\n",
    "    def reorder_past_batches(self, all_cache_key, all_cache_value, coordinates, beam_size):\n",
    "        \"\"\"[Reorder the input batch based on beam predictions\n",
    "        Future beams changes the best path order]\n",
    "\n",
    "        Args:\n",
    "            all_cache_key ([tf.tensor]): [K from Transformers]\n",
    "            all_cache_value ([tf.tensor]): [V from Transformers]\n",
    "            coordinates ([tf.tensor (bach_size x beam_size)]): [The order ]\n",
    "            beam_size ([int/tf.tensor]): [Number of beams]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "\n",
    "        \"\"\"\n",
    "        coordinates_reshaped = coordinates[:, :beam_size, -1] + tf.expand_dims(tf.range(tf.shape(coordinates)[0]) * beam_size, 1)\n",
    "        # Old Approach\n",
    "        # coordinates_reshaped = tf.reshape(coordinates_reshaped, -1)\n",
    "        # all_cache_key   = tf.gather(all_cache_key, coordinates_reshaped , axis=1)\n",
    "        # all_cache_value = tf.gather(all_cache_value, coordinates_reshaped, axis=1)\n",
    "\n",
    "        coordinates_reshaped = tf.reshape(coordinates_reshaped, (1,-1))\n",
    "        all_cache_key   = tf.squeeze(tf.gather(all_cache_key, coordinates_reshaped , axis=1), axis=1)\n",
    "        all_cache_value = tf.squeeze(tf.gather(all_cache_value, coordinates_reshaped, axis=1), axis=1)\n",
    "        return all_cache_key, all_cache_value\n",
    "\n",
    "\n",
    "    def greedy(self):\n",
    "        \"\"\"\n",
    "        This function will perform greedy decoding.\n",
    "        \"\"\"\n",
    "\n",
    "        # EOS check function\n",
    "        def cond(i, input_ids, all_cache_key, all_cache_value, past_length, initial_id):\n",
    "            eos_check = tf.greater(\n",
    "                tf.reduce_prod(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.cast(tf.equal(initial_id, self.eos_id), tf.int32), axis=[1]\n",
    "                    )\n",
    "                ),\n",
    "                0,\n",
    "            )\n",
    "            return tf.not_equal(eos_check, True)\n",
    "\n",
    "        def body(i, inputs_tuple, all_cache_key, all_cache_value, past_length, initial_id):\n",
    "\n",
    "            \"\"\"[This is the body of the beam decoder]\n",
    "\n",
    "            Args:\n",
    "                i ([tf.tensor]): [iterator (an int)]\n",
    "                inputs ([List of model inputs]): [description]\n",
    "                all_cache_key ([K]): [description]\n",
    "                all_cache_value ([V]): [description]\n",
    "                past_length ([tf.tensor (1 x batch_size)]): [description]\n",
    "                This is our main output or decoded ids]\n",
    "                alive_log_probs ([tf.tensor]): [To keep track of active ids]\n",
    "                alive_seq ([tf.tensor]): [description]\n",
    "\n",
    "            Returns:\n",
    "                [List of tensors]: [Outputs]\n",
    "            \"\"\"\n",
    "            inputs = {}\n",
    "            for k in range(len(self.input_name_list)):\n",
    "                inputs[self.input_name_list[k]] = inputs_tuple[k]\n",
    "\n",
    "            inputs['all_cache_key'] = all_cache_key\n",
    "            inputs['all_cache_value'] = all_cache_value\n",
    "            inputs['past_length'] = past_length\n",
    "\n",
    "            model_outputs = self.model(inputs)\n",
    "            model_logits = model_outputs['last_token_logits']\n",
    "\n",
    "            all_cache_key = model_outputs[\"all_cache_key\"]\n",
    "            all_cache_value = model_outputs[\"all_cache_value\"]\n",
    "            past_length = model_outputs[\"past_length\"]\n",
    "\n",
    "            prediction_ids = tf.argmax(model_logits, axis=1)\n",
    "            input_ids = tf.cast(tf.expand_dims(prediction_ids, axis=1), tf.int32)\n",
    "\n",
    "            inputs_tuple = [None] * len(self.input_name_list)\n",
    "\n",
    "            for index, name in self.input_name_map.items():\n",
    "                if name == \"input_ids\":\n",
    "                    inputs_tuple[index] = input_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_type_ids\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_type_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_mask\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_mask_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "            # Convert to tuple\n",
    "            inputs_tuple = tuple(inputs_tuple)\n",
    "\n",
    "            return [\n",
    "                i + 1,\n",
    "                inputs_tuple,\n",
    "                model_outputs[\"all_cache_key\"],\n",
    "                model_outputs[\"all_cache_value\"],\n",
    "                model_outputs[\"past_length\"],\n",
    "                tf.concat([initial_id, input_ids], axis=1),\n",
    "            ]\n",
    "\n",
    "        # @tf.function(experimental_relax_shapes=True)\n",
    "        def call_greedy(inputs):\n",
    "            input_ids_orig = inputs[\"input_ids\"]\n",
    "            # Original batch size and sequence length\n",
    "            batch_size = tf.shape(inputs['input_ids'])[0]\n",
    "            max_sequence_length = tf.shape(inputs['input_ids'])[1]\n",
    "            # Repeat for beam search (We nedd batch_size x beam_size)\n",
    "            model_inputs = {}\n",
    "            for input_key, input_value in inputs.items():\n",
    "                model_inputs[input_key] = input_value\n",
    "\n",
    "            # Pre-initialize addtional inputs\n",
    "            zero_entry  = tf.zeros((self.num_layers,\n",
    "                                    batch_size,\n",
    "                                    self.num_attention_heads,\n",
    "                                    max_sequence_length, self.attention_state))\n",
    "            all_cache_key   = zero_entry\n",
    "            all_cache_value = zero_entry\n",
    "            # past_length for keeping track of positional ids\n",
    "            past_length = tf.expand_dims(tf.zeros(batch_size, dtype=tf.int32), 0)\n",
    "            # Iterator to keep track of the loop\n",
    "            i = tf.constant([[0]])\n",
    "            initial_id = tf.ones(shape=(batch_size, 1), dtype=tf.int32)\n",
    "\n",
    "            # Add remaining model inputs\n",
    "            model_inputs['all_cache_key'] = all_cache_key\n",
    "            model_inputs['all_cache_value'] = all_cache_value\n",
    "            model_inputs['past_length'] = past_length\n",
    "\n",
    "            model_outputs = self.model(model_inputs)\n",
    "            model_logits = model_outputs[\"last_token_logits\"]\n",
    "            prediction_ids = tf.argmax(model_logits, axis=1)\n",
    "            input_ids = tf.cast(tf.expand_dims(prediction_ids, axis=1), tf.int32)\n",
    "\n",
    "            # Update iter\n",
    "            i = i + 1\n",
    "            all_cache_key = model_outputs[\"all_cache_key\"]\n",
    "            all_cache_value = model_outputs[\"all_cache_value\"]\n",
    "            initial_id = tf.concat([initial_id, input_ids], axis=1)\n",
    "\n",
    "            masks = tf.cast(tf.not_equal(inputs[\"input_ids\"], -1), tf.float32)\n",
    "            masks = tf.reshape(masks, (1, batch_size, 1, max_sequence_length, 1))\n",
    "            all_cache_key = all_cache_key * masks\n",
    "            all_cache_value = all_cache_value * masks\n",
    "\n",
    "            ## END\n",
    "            inputs_tuple = [None] * len(self.input_name_list)\n",
    "            input_shapes_tuple = [tf.TensorShape([None, None])] * len(self.input_name_list)\n",
    "            for index, name in self.input_name_map.items():\n",
    "                if name == \"input_ids\":\n",
    "                    inputs_tuple[index] = input_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_type_ids\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_type_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_mask\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_mask_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "\n",
    "            inputs_tuple = tuple(inputs_tuple)\n",
    "            input_shapes_tuple = tuple(input_shapes_tuple)\n",
    "\n",
    "            results = tf.while_loop(\n",
    "                cond,\n",
    "                body,\n",
    "                maximum_iterations=self.max_iterations - 1,\n",
    "                loop_vars=[\n",
    "                    i,\n",
    "                    inputs_tuple,\n",
    "                    all_cache_key,\n",
    "                    all_cache_value,\n",
    "                    model_outputs['past_length'],\n",
    "                    initial_id,\n",
    "                ],\n",
    "                shape_invariants=[\n",
    "                    i.get_shape(),\n",
    "                    input_shapes_tuple,\n",
    "                    tf.TensorShape(\n",
    "                        [\n",
    "                            self.num_layers,\n",
    "                            None,\n",
    "                            self.num_attention_heads,\n",
    "                            None,\n",
    "                            self.attention_state,\n",
    "                        ]\n",
    "                    ),\n",
    "                    tf.TensorShape(\n",
    "                        [\n",
    "                            self.num_layers,\n",
    "                            None,\n",
    "                            self.num_attention_heads,\n",
    "                            None,\n",
    "                            self.attention_state,\n",
    "                        ]\n",
    "                    ),\n",
    "                    tf.TensorShape([None, None]),\n",
    "                    tf.TensorShape([None, None]),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            results_dict = {}\n",
    "            results_dict[\"iterations\"] = results[0]\n",
    "            results_dict[\"input_ids\"] = input_ids_orig\n",
    "            # Skip -1 initial ids\n",
    "            results_dict[\"predicted_ids\"] = results[-1][:, 1:]\n",
    "            # Add matched positions here\n",
    "            matched_positions = tf.argmax(tf.cast(tf.equal(self.eos_id,\n",
    "                                                       results_dict[\"predicted_ids\"]), tf.int64), axis=1)\n",
    "            # no eos matched positions will be 0, replace with -1\n",
    "            eos_pos_mask = tf.cast(tf.equal(matched_positions, 0), tf.int64) * -1\n",
    "            matched_positions = tf.cast(matched_positions, tf.int64) + eos_pos_mask\n",
    "            results_dict['matched_eos_pos'] = matched_positions\n",
    "            results_dict[\"predicted_ids\"] = tf.expand_dims(results[-1][:, 1:], 1)\n",
    "\n",
    "            return results_dict\n",
    "\n",
    "        return call_greedy\n",
    "\n",
    "\n",
    "    def beam(self):\n",
    "        \"\"\"\n",
    "        This function will perform beam decoding.\n",
    "        \"\"\"\n",
    "\n",
    "        # EOS check function\n",
    "        def cond(i, input_ids, all_cache_key, all_cache_value, past_length,  alive_log_probs, alive_seq):\n",
    "            eos_check = tf.greater(tf.reduce_prod(tf.reduce_sum(tf.cast(tf.equal(alive_seq, self.eos_id),\n",
    "                                                                tf.int32), axis=[2])), 0)\n",
    "            return tf.not_equal(eos_check, True)\n",
    "\n",
    "        def body(i,\n",
    "                 inputs_tuple,\n",
    "                 all_cache_key,\n",
    "                 all_cache_value, past_length, alive_log_probs, alive_seq):\n",
    "            \"\"\"[This is the body of the beam decoder]\n",
    "\n",
    "            Args:\n",
    "                i ([tf.tensor]): [iterator (an int)]\n",
    "                inputs ([List of model inputs]): [description]\n",
    "                all_cache_key ([K]): [description]\n",
    "                all_cache_value ([V]): [description]\n",
    "                past_length ([tf.tensor (1 x batch_size)]): [description]\n",
    "                This is our main output or decoded ids]\n",
    "                alive_log_probs ([tf.tensor]): [To keep track of active ids]\n",
    "                alive_seq ([tf.tensor]): [description]\n",
    "\n",
    "            Returns:\n",
    "                [List of tensors]: [Outputs]\n",
    "            \"\"\"\n",
    "            inputs = {}\n",
    "            for k in range(len(self.input_name_list)):\n",
    "                inputs[self.input_name_list[k]] = inputs_tuple[k]\n",
    "            inputs['all_cache_key'] = all_cache_key\n",
    "            inputs['all_cache_value'] = all_cache_value\n",
    "            inputs['past_length'] = past_length\n",
    "\n",
    "            beams_to_keep = 2 * self.beam_size\n",
    "            model_outputs = self.model(inputs)\n",
    "\n",
    "            model_logits = model_outputs['last_token_logits']\n",
    "\n",
    "            all_cache_key = model_outputs[\"all_cache_key\"]\n",
    "            all_cache_value = model_outputs[\"all_cache_value\"]\n",
    "            past_length = model_outputs[\"past_length\"]\n",
    "\n",
    "            if self.top_k > 0:\n",
    "                model_logits = top_k_logits(model_logits, k=self.top_k)\n",
    "            if self.top_p > 0:\n",
    "                model_logits = top_p_logits(model_logits, p=self.top_p)\n",
    "\n",
    "            vocab_size = tf.shape(model_logits)[1]\n",
    "            batch_size = tf.shape(inputs['input_ids'])[0]//self.beam_size\n",
    "            logits = tf.reshape(model_logits, (batch_size, self.beam_size, -1))\n",
    "            # # Convert logits to normalized log probs\n",
    "            candidate_log_probs = _log_prob_from_logits(logits)\n",
    "\n",
    "            # Calculate new log probabilities if each of the alive sequences were\n",
    "            # extended # by the the candidate IDs.\n",
    "            # Shape [batch_size, beam_size, vocab_size]\n",
    "            log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, 2)\n",
    "\n",
    "            # Calculate new log probabilities if each of the alive sequences were\n",
    "            # extended # by the the candidate IDs.\n",
    "            # Shape [batch_size, beam_size, vocab_size]\n",
    "            log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)\n",
    "\n",
    "            # Each batch item has beam_size * vocab_size candidate sequences. For each\n",
    "            # batch item, get the k candidates with the highest log probabilities.\n",
    "            flat_log_probs = tf.reshape(log_probs,\n",
    "                                        [-1, self.beam_size * vocab_size])\n",
    "\n",
    "\n",
    "            if self.do_sample:\n",
    "                next_tokens = tf.random.categorical(\n",
    "                    flat_log_probs, dtype=tf.int32, num_samples=beams_to_keep\n",
    "                )  # (batch_size, 2 * num_beams)\n",
    "\n",
    "                # # Compute next scores\n",
    "                next_scores = tf.gather(flat_log_probs, next_tokens, batch_dims=1)  # (batch_size, 2 * num_beams)\n",
    "\n",
    "                # # sort the sampled vector to make sure that the first num_beams samples are the best\n",
    "                next_scores_indices = tf.argsort(next_scores, direction=\"DESCENDING\", axis=1)\n",
    "                next_scores = tf.gather(next_scores, next_scores_indices, batch_dims=1)  # (batch_size, num_beams * 2)\n",
    "                next_tokens = tf.gather(next_tokens, next_scores_indices, batch_dims=1)  # (batch_size, num_beams * 2)\n",
    "\n",
    "                topk_log_probs = next_scores\n",
    "                topk_indices   = next_tokens\n",
    "            else:\n",
    "                topk_log_probs, topk_indices = tf.nn.top_k(flat_log_probs, k=beams_to_keep)\n",
    "\n",
    "            topk_beam_indices = topk_indices // vocab_size\n",
    "            topk_seq, coordinates = _gather_beams(\n",
    "                alive_seq, topk_beam_indices, batch_size,\n",
    "                beams_to_keep)\n",
    "            topk_seq = tf.cast(topk_seq, tf.int32)\n",
    "            topk_ids = topk_indices % vocab_size\n",
    "            topk_seq = tf.concat([topk_seq, tf.expand_dims(topk_ids, axis=2)], axis=2)\n",
    "\n",
    "            topk_alive_seq  = topk_seq[:, :self.beam_size, :]\n",
    "            alive_log_probs = topk_log_probs[:, :self.beam_size]\n",
    "            input_ids = tf.reshape(topk_ids[:, :self.beam_size], [-1, 1])\n",
    "            alive_seq = topk_alive_seq\n",
    "\n",
    "            inputs_tuple = [None] * len(self.input_name_list)\n",
    "\n",
    "            for index, name in self.input_name_map.items():\n",
    "                if name == \"input_ids\":\n",
    "                    inputs_tuple[index] = input_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_type_ids\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_type_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_mask\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_mask_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "            # Convert to tuple\n",
    "            inputs_tuple = tuple(inputs_tuple)\n",
    "\n",
    "            all_cache_key , all_cache_value = self.reorder_past_batches(all_cache_key, all_cache_value, coordinates, self.beam_size)\n",
    "            model_outputs[\"all_cache_key\"] = all_cache_key\n",
    "            model_outputs[\"all_cache_value\"] = all_cache_value\n",
    "\n",
    "            return [\n",
    "                i + 1,\n",
    "                inputs_tuple,\n",
    "                model_outputs[\"all_cache_key\"],\n",
    "                model_outputs[\"all_cache_value\"],\n",
    "                model_outputs[\"past_length\"],\n",
    "                alive_log_probs,\n",
    "                alive_seq\n",
    "            ]\n",
    "\n",
    "        # @tf.function(experimental_relax_shapes=True)\n",
    "        def call_beam(inputs):\n",
    "            \"\"\"The main function to perform beam search\n",
    "            Args:\n",
    "                inputs ([dict]): [dict of tf.tensors (model inputs)]\n",
    "            \"\"\"\n",
    "            input_ids_orig = inputs[\"input_ids\"]\n",
    "            # We take 2x beams\n",
    "            beams_to_keep = 2 * self.beam_size\n",
    "            # Original batch size and sequence length\n",
    "            batch_size = tf.shape(inputs['input_ids'])[0]\n",
    "            max_sequence_length = tf.shape(inputs['input_ids'])[1]\n",
    "            # Repeat for beam search (We nedd batch_size x beam_size)\n",
    "            model_inputs = {}\n",
    "            for input_key, input_value in inputs.items():\n",
    "                model_inputs[input_key] = tf.repeat(input_value, [self.beam_size], axis=0)\n",
    "            # New batch size\n",
    "            batch_size_updated = tf.shape(model_inputs['input_ids'])[0]\n",
    "\n",
    "\n",
    "            # Pre-initialize addtional inputs\n",
    "            zero_entry  = tf.zeros((self.num_layers,\n",
    "                                    batch_size_updated,\n",
    "                                    self.num_attention_heads,\n",
    "                                    max_sequence_length, self.attention_state))\n",
    "            all_cache_key   = zero_entry\n",
    "            all_cache_value = zero_entry\n",
    "            # past_length for keeping track of positional ids\n",
    "            past_length = tf.expand_dims(tf.zeros(batch_size_updated, dtype=tf.int32), 0)\n",
    "            # Iterator to keep track of the loop\n",
    "            i = tf.constant([[0]])\n",
    "\n",
    "            # Add remaining model inputs\n",
    "            model_inputs['all_cache_key'] = all_cache_key\n",
    "            model_inputs['all_cache_value'] = all_cache_value\n",
    "            model_inputs['past_length'] = past_length\n",
    "\n",
    "            # We need this to re-ordering and keep track of best -log(prob))\n",
    "            alive_log_probs = -np.inf * tf.ones((batch_size, self.beam_size-1))\n",
    "            alive_log_probs = tf.concat([tf.zeros([batch_size, 1]), alive_log_probs], axis=1)\n",
    "            alive_seq = tf.zeros((batch_size, self.beam_size, 1))\n",
    "\n",
    "            # First pass to the model\n",
    "            model_outputs = self.model(model_inputs)\n",
    "            model_logits = model_outputs['last_token_logits']\n",
    "            # Update iter\n",
    "            i = i + 1\n",
    "            all_cache_key = model_outputs[\"all_cache_key\"]\n",
    "            all_cache_value = model_outputs[\"all_cache_value\"]\n",
    "            past_length = model_outputs[\"past_length\"]\n",
    "\n",
    "            if self.top_k > 0:\n",
    "                model_logits = top_k_logits(model_logits, k=self.top_k)\n",
    "            if self.top_p > 0:\n",
    "                model_logits = top_p_logits(model_logits, p=self.top_p)\n",
    "\n",
    "            # vocab size\n",
    "            vocab_size = tf.shape(model_logits)[1]\n",
    "            logits = tf.reshape(model_logits, (batch_size, self.beam_size, -1))\n",
    "            # # Convert logits to normalized log probs\n",
    "            candidate_log_probs = _log_prob_from_logits(logits)\n",
    "\n",
    "            # Calculate new log probabilities if each of the alive sequences were\n",
    "            # extended # by the the candidate IDs.\n",
    "            # Shape [batch_size, beam_size, vocab_size]\n",
    "            log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, 2)\n",
    "\n",
    "            # Calculate new log probabilities if each of the alive sequences were\n",
    "            # extended # by the the candidate IDs.\n",
    "            # Shape [batch_size, beam_size, vocab_size]\n",
    "            log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)\n",
    "\n",
    "            # Each batch item has beam_size * vocab_size candidate sequences. For each\n",
    "            # batch item, get the k candidates with the highest log probabilities.\n",
    "            flat_log_probs = tf.reshape(log_probs,\n",
    "                                        [-1, self.beam_size * vocab_size])\n",
    "\n",
    "\n",
    "            if self.do_sample:\n",
    "                next_tokens = tf.random.categorical(\n",
    "                    flat_log_probs, dtype=tf.int32, num_samples=beams_to_keep\n",
    "                )  # (batch_size, 2 * num_beams)\n",
    "\n",
    "                # # Compute next scores\n",
    "                next_scores = tf.gather(flat_log_probs, next_tokens, batch_dims=1)  # (batch_size, 2 * num_beams)\n",
    "\n",
    "                # # sort the sampled vector to make sure that the first num_beams samples are the best\n",
    "                next_scores_indices = tf.argsort(next_scores, direction=\"DESCENDING\", axis=1)\n",
    "                next_scores = tf.gather(next_scores, next_scores_indices, batch_dims=1)  # (batch_size, num_beams * 2)\n",
    "                next_tokens = tf.gather(next_tokens, next_scores_indices, batch_dims=1)  # (batch_size, num_beams * 2)\n",
    "\n",
    "                topk_log_probs = next_scores\n",
    "                topk_indices   = next_tokens\n",
    "            else:\n",
    "                topk_log_probs, topk_indices = tf.nn.top_k(flat_log_probs, k=beams_to_keep)\n",
    "\n",
    "            topk_beam_indices = topk_indices // vocab_size\n",
    "            topk_seq, coordinates = _gather_beams(\n",
    "                alive_seq, topk_beam_indices, batch_size,\n",
    "                beams_to_keep)\n",
    "            topk_seq = tf.cast(topk_seq, tf.int32)\n",
    "            topk_ids = topk_indices % vocab_size\n",
    "            topk_seq = tf.concat([topk_seq, tf.expand_dims(topk_ids, axis=2)], axis=2)\n",
    "\n",
    "            topk_alive_seq  = topk_seq[:, :self.beam_size, :]\n",
    "            alive_log_probs = topk_log_probs[:, :self.beam_size]\n",
    "            input_ids = tf.reshape(topk_ids[:, :self.beam_size], [-1, 1])\n",
    "            alive_seq = topk_alive_seq\n",
    "\n",
    "            inputs_tuple = [None] * len(self.input_name_list)\n",
    "            input_shapes_tuple = [tf.TensorShape([None, None])] * len(self.input_name_list)\n",
    "            for index, name in self.input_name_map.items():\n",
    "                if name == \"input_ids\":\n",
    "                    inputs_tuple[index] = input_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_type_ids\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_type_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_mask\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_mask_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "\n",
    "            inputs_tuple = tuple(inputs_tuple)\n",
    "            input_shapes_tuple = tuple(input_shapes_tuple)\n",
    "\n",
    "            # on step 0\n",
    "\n",
    "            masks = tf.cast(tf.not_equal(model_inputs['input_ids'], -1), tf.float32)\n",
    "            masks = tf.reshape(masks, (1, batch_size_updated, 1, tf.shape(model_inputs['input_ids'])[1], 1))\n",
    "            all_cache_key = all_cache_key * masks\n",
    "            all_cache_value = all_cache_value * masks\n",
    "\n",
    "            all_cache_key , all_cache_value = self.reorder_past_batches(all_cache_key, all_cache_value, coordinates, self.beam_size)\n",
    "\n",
    "            ## END\n",
    "            results = tf.while_loop(\n",
    "                cond,\n",
    "                body,\n",
    "                maximum_iterations=self.max_iterations - 1,\n",
    "                loop_vars=[\n",
    "                    i,\n",
    "                    inputs_tuple,\n",
    "                    all_cache_key,\n",
    "                    all_cache_value,\n",
    "                    past_length,\n",
    "                    alive_log_probs,\n",
    "                    alive_seq\n",
    "                ],\n",
    "                shape_invariants=[\n",
    "                    i.get_shape(),\n",
    "                    input_shapes_tuple,\n",
    "                    tf.TensorShape(\n",
    "                        [\n",
    "                            self.num_layers,\n",
    "                            None,\n",
    "                            self.num_attention_heads,\n",
    "                            None,\n",
    "                            self.attention_state,\n",
    "                        ]\n",
    "                    ),\n",
    "                    tf.TensorShape(\n",
    "                        [\n",
    "                            self.num_layers,\n",
    "                            None,\n",
    "                            self.num_attention_heads,\n",
    "                            None,\n",
    "                            self.attention_state,\n",
    "                        ]\n",
    "                    ),\n",
    "                    tf.TensorShape([None, None]),\n",
    "                    tf.TensorShape([None, None]),\n",
    "                    tf.TensorShape([None, None,None])\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            results_dict = {}\n",
    "            results_dict[\"iterations\"] = results[0]\n",
    "            results_dict[\"input_ids\"] = input_ids_orig\n",
    "            # Skip -1 initial ids\n",
    "            results_dict[\"predicted_ids\"] = results[-1][:,:,1:] # to remove initial 0\n",
    "\n",
    "            matched_positions = tf.squeeze(tf.reshape(tf.argmax(tf.cast(tf.equal(self.eos_id,\n",
    "                                results_dict[\"predicted_ids\"]), tf.int32), axis=2),\n",
    "                                (-1, batch_size * self.beam_size)), [0]) -1\n",
    "            # no eos matched positions will be 0, replace with -1\n",
    "            eos_pos_mask = tf.cast(tf.equal(matched_positions, 0), tf.int32) * -1\n",
    "            matched_positions = tf.cast(matched_positions, tf.int32) + eos_pos_mask\n",
    "            results_dict[\"matched_eos_pos\"] = matched_positions\n",
    "\n",
    "\n",
    "            return results_dict\n",
    "\n",
    "        return call_beam\n",
    "\n",
    "    def top_k_top(self):\n",
    "\n",
    "        # EOS check function\n",
    "        def cond(i, input_ids, all_cache_key, all_cache_value, past_length, initial_id):\n",
    "            eos_check = tf.greater(\n",
    "                tf.reduce_prod(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.cast(tf.equal(initial_id, self.eos_id), tf.int32), axis=[1]\n",
    "                    )\n",
    "                ),\n",
    "                0,\n",
    "            )\n",
    "            return tf.not_equal(eos_check, True)\n",
    "\n",
    "        def body(i, inputs_tuple, all_cache_key, all_cache_value, past_length, initial_id):\n",
    "            \"\"\"[This is the body of the top k top p decoder]\n",
    "\n",
    "            Args:\n",
    "                i ([tf.tensor]): [iterator (an int)]\n",
    "                inputs ([List of model inputs]): [description]\n",
    "                all_cache_key ([K]): [description]\n",
    "                all_cache_value ([V]): [description]\n",
    "                past_length ([tf.tensor (1 x batch_size)]): [description]\n",
    "                This is our main output or decoded ids]\n",
    "                initial_id ([tf.tensor]): [To keep track of concatanted ids generated in each iteration]\n",
    "\n",
    "            Returns:\n",
    "                [List of tensors]: [Outputs]\n",
    "            \"\"\"\n",
    "            inputs = {}\n",
    "            for k in range(len(self.input_name_list)):\n",
    "                inputs[self.input_name_list[k]] = inputs_tuple[k]\n",
    "            inputs['all_cache_key'] = all_cache_key\n",
    "            inputs['all_cache_value'] = all_cache_value\n",
    "            inputs['past_length'] = past_length\n",
    "\n",
    "            model_outputs = self.model(inputs)\n",
    "            model_logits = model_outputs['last_token_logits']\n",
    "\n",
    "            if self.top_k > 0:\n",
    "                model_logits = top_k_logits(model_logits, k=self.top_k)\n",
    "            if self.top_p > 0:\n",
    "                model_logits = top_p_logits(model_logits, p=self.top_p)\n",
    "\n",
    "            if self.do_sample:\n",
    "                prediction_ids = tf.random.categorical(model_logits, num_samples=1)\n",
    "                input_ids = tf.cast(prediction_ids, tf.int32)\n",
    "            else:\n",
    "                prediction_ids = tf.argmax(model_logits, axis=1)\n",
    "                input_ids = tf.cast(tf.expand_dims(prediction_ids, axis=1), tf.int32)\n",
    "\n",
    "            inputs_tuple = [None] * len(self.input_name_list)\n",
    "\n",
    "            for index, name in self.input_name_map.items():\n",
    "                if name == \"input_ids\":\n",
    "                    inputs_tuple[index] = input_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_type_ids\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_type_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_mask\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_mask_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "            # Convert to tuple\n",
    "            inputs_tuple = tuple(inputs_tuple)\n",
    "            return [\n",
    "                i + 1,\n",
    "                inputs_tuple,\n",
    "                model_outputs[\"all_cache_key\"],\n",
    "                model_outputs[\"all_cache_value\"],\n",
    "                model_outputs[\"past_length\"],\n",
    "                tf.concat([initial_id, input_ids], axis=1),\n",
    "            ]\n",
    "\n",
    "        def call_top_k_top_p(inputs):\n",
    "            \"\"\"The main function to perform Top K top P (Nucleus) decoding\n",
    "            Args:\n",
    "                inputs ([dict]): [dict of tf.tensors (model inputs)]\n",
    "            \"\"\"\n",
    "            input_ids_orig = inputs[\"input_ids\"]\n",
    "            batch_size = tf.shape(inputs['input_ids'])[0]\n",
    "            max_sequence_length = tf.shape(inputs['input_ids'])[1]\n",
    "            model_inputs = {}\n",
    "            for input_key, input_value in inputs.items():\n",
    "                model_inputs[input_key] = tf.repeat(input_value, [self.num_return_sequences], axis=0)\n",
    "            # Updated batch size\n",
    "            batch_size_updated = tf.shape(model_inputs['input_ids'])[0]\n",
    "\n",
    "            # Pre-initialize addtional inputs\n",
    "            zero_entry  = tf.zeros((self.num_layers,\n",
    "                                    batch_size_updated,\n",
    "                                    self.num_attention_heads,\n",
    "                                    max_sequence_length, self.attention_state))\n",
    "            all_cache_key   = zero_entry\n",
    "            all_cache_value = zero_entry\n",
    "            # past_length for keeping track of positional ids\n",
    "            past_length = tf.expand_dims(tf.zeros(batch_size_updated, dtype=tf.int32), 0)\n",
    "            # Iterator to keep track of the loop\n",
    "            i = tf.constant([[0]])\n",
    "            initial_id = tf.ones(shape=(batch_size_updated, 1), dtype=tf.int32)\n",
    "\n",
    "            # Add remaining model inputs\n",
    "            model_inputs['all_cache_key'] = all_cache_key\n",
    "            model_inputs['all_cache_value'] = all_cache_value\n",
    "            model_inputs['past_length'] = past_length\n",
    "\n",
    "            # First pass to the model\n",
    "            model_outputs = self.model(model_inputs)\n",
    "            model_logits = model_outputs['last_token_logits']\n",
    "\n",
    "            if self.top_k > 0:\n",
    "                model_logits = top_k_logits(model_logits, k=self.top_k)\n",
    "            if self.top_p > 0:\n",
    "                model_logits = top_p_logits(model_logits, p=self.top_p)\n",
    "\n",
    "            if self.do_sample:\n",
    "                prediction_ids = tf.random.categorical(model_logits, num_samples=1)\n",
    "                input_ids = tf.cast(prediction_ids, tf.int32)\n",
    "            else:\n",
    "                prediction_ids = tf.argmax(model_logits, axis=1)\n",
    "                input_ids = tf.cast(tf.expand_dims(prediction_ids, axis=1), tf.int32)\n",
    "            inputs_tuple = [None] * len(self.input_name_list)\n",
    "            input_shapes_tuple = [tf.TensorShape([None, None])] * len(self.input_name_list)\n",
    "            for index, name in self.input_name_map.items():\n",
    "                if name == \"input_ids\":\n",
    "                    inputs_tuple[index] = input_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_type_ids\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_type_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "                if name == \"input_mask\":\n",
    "                    inputs_tuple[index] = tf.ones_like(input_ids) * self.input_mask_ids\n",
    "                    # input_shapes_tuple.append(tf.TensorShape([None, None]))\n",
    "                    continue\n",
    "\n",
    "            inputs_tuple = tuple(inputs_tuple)\n",
    "            input_shapes_tuple = tuple(input_shapes_tuple)\n",
    "\n",
    "            # Concatanate\n",
    "            initial_id = tf.concat([initial_id, input_ids], axis=1)\n",
    "\n",
    "            # on step 0\n",
    "\n",
    "            masks = tf.cast(tf.not_equal(model_inputs['input_ids'], -1), tf.float32)\n",
    "            masks = tf.reshape(masks, (1, batch_size_updated, 1, tf.shape(model_inputs['input_ids'])[1], 1))\n",
    "\n",
    "            all_cache_key = model_outputs['all_cache_key']\n",
    "            all_cache_value = model_outputs['all_cache_value']\n",
    "            all_cache_key = all_cache_key * masks\n",
    "            all_cache_value = all_cache_value * masks\n",
    "            ## END\n",
    "\n",
    "            results = tf.while_loop(\n",
    "                cond,\n",
    "                body,\n",
    "                maximum_iterations=self.max_iterations - 1,\n",
    "                loop_vars=[\n",
    "                    i,\n",
    "                    inputs_tuple,\n",
    "                    all_cache_key,\n",
    "                    all_cache_value,\n",
    "                    model_outputs['past_length'],\n",
    "                    initial_id,\n",
    "                ],\n",
    "                shape_invariants=[\n",
    "                    i.get_shape(),\n",
    "                    input_shapes_tuple,\n",
    "                    tf.TensorShape(\n",
    "                        [\n",
    "                            self.num_layers,\n",
    "                            None,\n",
    "                            self.num_attention_heads,\n",
    "                            None,\n",
    "                            self.attention_state,\n",
    "                        ]\n",
    "                    ),\n",
    "                    tf.TensorShape(\n",
    "                        [\n",
    "                            self.num_layers,\n",
    "                            None,\n",
    "                            self.num_attention_heads,\n",
    "                            None,\n",
    "                            self.attention_state,\n",
    "                        ]\n",
    "                    ),\n",
    "                    tf.TensorShape([None, None]),\n",
    "                    tf.TensorShape([None, None]),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            results_dict = {}\n",
    "            results_dict[\"iterations\"] = results[0]\n",
    "            results_dict[\"input_ids\"] = input_ids_orig\n",
    "            # Skip -1 initial ids\n",
    "            results_dict[\"predicted_ids\"] = results[-1][:, 1:]\n",
    "            results_dict[\"predicted_ids\"] = tf.reshape(results_dict[\"predicted_ids\"],(batch_size, self.num_return_sequences, -1))\n",
    "\n",
    "            matched_positions = tf.squeeze(tf.reshape(tf.argmax(tf.cast(tf.equal(self.eos_id,\n",
    "                                results_dict[\"predicted_ids\"]), tf.int32), axis=2),\n",
    "                                (-1, batch_size * self.num_return_sequences)), [0]) -1\n",
    "            # no eos matched positions will be 0, replace with -1\n",
    "            eos_pos_mask = tf.cast(tf.equal(matched_positions, 0), tf.int32) * -1\n",
    "            matched_positions = tf.cast(matched_positions, tf.int32) + eos_pos_mask\n",
    "            results_dict[\"matched_eos_pos\"] = matched_positions\n",
    "\n",
    "            return results_dict\n",
    "        return call_top_k_top_p\n",
    "\n",
    "    def call(self, inputs):\n",
    "        results_dict = self.decoder_fn(inputs)\n",
    "        return results_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 3.0038630962371826 seconds\n",
      "INFO:tensorflow:Assets written to: model_temp_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_temp_pb/assets\n"
     ]
    }
   ],
   "source": [
    "decoder_layer_serializable = TextDecoderSerializable(\n",
    "    model_tf_transformers,\n",
    "    input_name_list=[\"input_ids\", \"input_mask\", \"input_type_ids\"],\n",
    "    max_iterations=25,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    attention_state=64,\n",
    "    mode=\"beam\",\n",
    "    do_sample=False,\n",
    "    beam_size=2,\n",
    "    eos_id=-100,\n",
    "    input_mask_ids=1,\n",
    "    input_type_ids=0,\n",
    ")\n",
    "\n",
    "inputs_for_serializable = tokenizer_fn(text_list)\n",
    "\n",
    "main_inputs = {}\n",
    "for k, v in inputs_for_serializable.items():\n",
    "    main_inputs[k] = tf.ragged.constant(v)\n",
    "main_inputs['input_ids'] = main_inputs['input_ids'].to_tensor(-1)\n",
    "main_inputs['input_mask'] = main_inputs['input_mask'].to_tensor(0)\n",
    "main_inputs['input_type_ids'] = main_inputs['input_type_ids'].to_tensor(0)\n",
    "\n",
    "start_time = time.time()\n",
    "results_serializable_beam = decoder_layer_serializable(main_inputs)\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "\n",
    "decoder_model  = decoder_layer_serializable.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(\"model_temp_pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 2.9446468353271484 seconds\n",
      "INFO:tensorflow:Assets written to: model_temp_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_temp_pb/assets\n"
     ]
    }
   ],
   "source": [
    "decoder_layer_serializable = TextDecoderSerializable(\n",
    "    model_tf_transformers,\n",
    "    input_name_list=[\"input_ids\", \"input_mask\", \"input_type_ids\"],\n",
    "    max_iterations=25,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    attention_state=64,\n",
    "    mode=\"top_k_top_p\",\n",
    "    do_sample=False,\n",
    "    num_return_sequences=2,\n",
    "    top_k = 50,\n",
    "    top_p = 0.7,\n",
    "    eos_id=-100,\n",
    "    input_mask_ids=1,\n",
    "    input_type_ids=0,\n",
    ")\n",
    "\n",
    "inputs_for_serializable = tokenizer_fn(text_list)\n",
    "\n",
    "main_inputs = {}\n",
    "for k, v in inputs_for_serializable.items():\n",
    "    main_inputs[k] = tf.ragged.constant(v)\n",
    "main_inputs['input_ids'] = main_inputs['input_ids'].to_tensor(-1)\n",
    "main_inputs['input_mask'] = main_inputs['input_mask'].to_tensor(0)\n",
    "main_inputs['input_type_ids'] = main_inputs['input_type_ids'].to_tensor(0)\n",
    "\n",
    "start_time = time.time()\n",
    "results_serializable_top_k_top_p = decoder_layer_serializable(main_inputs)\n",
    "end_time = time.time()\n",
    "print(\"Time taken {} seconds\".format(end_time - start_time))\n",
    "\n",
    "decoder_model  = decoder_layer_serializable.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(\"model_temp_pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.assert_equal(result_greedy['predicted_ids'], results_serializable_greedy['predicted_ids'])\n",
    "tf.assert_equal(result_beam['predicted_ids'], results_serializable_beam['predicted_ids'])\n",
    "tf.assert_equal(results_serializable_top_k_top_p['predicted_ids'], result_top_k_top_p['predicted_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
