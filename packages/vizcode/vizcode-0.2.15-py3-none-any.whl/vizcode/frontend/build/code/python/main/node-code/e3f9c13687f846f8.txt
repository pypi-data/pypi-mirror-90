def remove_project_from_article():    """    Removes project from this article    :param: JWT token for a owner, article link, project id    :return: Success if project removed from this article    """    username = get_jwt_identity()    exists = Owner.query.filter_by(username=username).first()    if exists:        article_link = request.form.get('article_link')        project_id = int(request.form.get('project_id'))        article = Article.query.filter_by(article_link=article_link).first()        if article:            ids = article.get_project_ids()            for i in range(len(ids)):                if ids[i] == project_id:                    ids[i] = None            ids = sorted(ids, key=lambda x: x is None)  # move None's to back            article.project_id1 = ids[0]            article.project_id2 = ids[1]            article.project_id3 = ids[2]            article.project_id4 = ids[3]            article.project_id5 = ids[4]            article.project_id6 = ids[5]            if username == 'admin':                article.edited_by_newspark = True            else:                article.edited_by_publisher = True            db.session.commit()            sql_query = '''select project_id                            from projects, organizations                            where projects.organization_id=organizations.email                            and projects.removed=FALSE                            and organizations.verified=TRUE;'''            conn = db.engine.connect().connection            df = pd.read_sql(sql_query, conn)            other_ids = list(filter(lambda x: not (x in ids), list(df['project_id'])))            # store recommendations in a json file and upload to a s3 bucket            update_recommendations(s3_client=s3, article_link=article_link, ids=ids, other_ids=other_ids)            # Re-run the commands to get the right data for the articles in the cache            # TODO: set up a celery worker to do all of this            if redis.exists(article_link):                redis.delete(article_link)            project_info_list = get_projects_from_article(article_url=article_link,                                                          num_ids=application.config['NUM_CHOICES'])            # Save to redis cache            redis.set(article_link, pickle.dumps(project_info_list))            return jsonify("Success")        else:            return jsonify("Article does not exist.")    else:        return jsonify("Publisher does not exist.")